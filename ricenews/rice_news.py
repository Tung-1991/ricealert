# -*- coding: utf-8 -*-
# rice_news.py
# Version: 6.0 (Scoring Engine)
# Description: N√¢ng c·∫•p ho√†n to√†n logic ph√¢n lo·∫°i tin t·ª©c. Thay v√¨ ch·ªâ d·ª±a v√†o
#              t·ª´ kh√≥a ƒë·∫ßu ti√™n t√¨m th·∫•y, phi√™n b·∫£n n√†y s·ª≠ d·ª•ng m·ªôt h·ªá th·ªëng
#              ch·∫•m ƒëi·ªÉm (scoring). M·ªói t·ª´ kh√≥a c√≥ m·ªôt tr·ªçng s·ªë ƒëi·ªÉm d∆∞∆°ng (t√≠ch c·ª±c)
#              ho·∫∑c √¢m (ti√™u c·ª±c). T·ªïng ƒëi·ªÉm c·ªßa m·ªôt tin t·ª©c s·∫Ω quy·∫øt ƒë·ªãnh
#              m·ª©c ƒë·ªô quan tr·ªçng (Level) c·ªßa n√≥, gi√∫p ƒë√°nh gi√° ch√≠nh x√°c h∆°n.
#              Output JSON gi·ªù s·∫Ω c√≥ th√™m tr∆∞·ªùng `news_score`.

import os
import json
import requests
import feedparser
import hashlib
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import time
import re
from typing import List, Dict, Tuple
from collections import defaultdict, Counter

# Import market_context functions
from market_context import get_market_context_data, get_market_context

# ==============================================================================
# CONFIG & SETUP (C√ì THAY ƒê·ªîI)
# ==============================================================================
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
load_dotenv(dotenv_path=dotenv_path)

VN_TZ = timezone(timedelta(hours=7))
BASE_DIR = os.getenv("RICENEWS_BASE_DIR", "/root/ricealert/ricenews")
LOG_DIR = os.path.join(BASE_DIR, "lognew")
COOLDOWN_TRACKER = os.path.join(BASE_DIR, "cooldown_tracker.json")
DISCORD_WEBHOOK = os.getenv("DISCORD_NEWS_WEBHOOK")
os.makedirs(LOG_DIR, exist_ok=True)

WATCHED_COINS = set([s.replace("USDT", "").lower() for s in os.getenv("SYMBOLS", "").split(",")])
WATCHED_COINS.update(["btc", "eth", "xrp", "sol", "bnb"])

LEVEL_COOLDOWN_MINUTES = {
    "CRITICAL": 15, "WARNING": 45, "ALERT": 90,
    "WATCHLIST": 180, "INFO": 360
}
SUMMARY_COOLDOWN_SECONDS = 6 * 60 * 60

# *** N√ÇNG C·∫§P L·ªöN 1: H·ªÜ TH·ªêNG ƒêI·ªÇM CHO T·ª™ KH√ìA ***
# ƒêi·ªÉm d∆∞∆°ng = T√≠ch c·ª±c, ƒêi·ªÉm √¢m = Ti√™u c·ª±c
# ƒê·ªô l·ªõn c·ªßa ƒëi·ªÉm th·ªÉ hi·ªán m·ª©c ƒë·ªô t√°c ƒë·ªông
KEYWORD_SCORES = {
    # Critical (R·∫•t quan tr·ªçng)
    "etf approval": 10, "etf approved": 10, "regulatory approval": 10, "will list": 8, "listing": 7,
    "halving": 8, "fomc": 7, "interest rate": 7, "cpi": 7, "war": -9, "sec sues": -9, "sec charges": -9,
    "hack": -10, "exploit": -10, "lawsuit": -8, "delist": -8, "b·ªã ƒëi·ªÅu tra": -9, "ki·ªán": -8, "downtime": -7, "outage": -7,
    "unlock": -6,

    # Alert (Quan tr·ªçng)
    "partnership": 5, "mainnet": 6, "upgrade": 5, "launch": 5, "adoption": 5,
    "margin": 4, "futures": 4, "available on": 4, "will add": 4,
    "maintenance": -5, "regulation": -6,

    # Watchlist (ƒê√°ng ch√∫ √Ω)
    "testnet": 3, "airdrop": 3, "voting": 2, "ama": 1, "token burn": 4, "governance": 2, "tvl hits record": 5,

    # Modifiers (T·ª´ b·ªï nghƒ©a - tƒÉng/gi·∫£m ƒëi·ªÉm)
    "record": 2, "huge": 2, "massive": 2, "significant": 2,
    "major": 2, "minor": -1, "delay": -3, "vulnerability": -7
}

# *** N√ÇNG C·∫§P L·ªöN 2: NG∆Ø·ª†NG ƒêI·ªÇM ƒê·ªÇ PH√ÇN LO·∫†I ***
SCORE_THRESHOLDS = {
    "CRITICAL": 8,
    "WARNING": 5,  # Ng∆∞·ª°ng cho tin ti√™u c·ª±c (ƒëi·ªÉm < -5) s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω ri√™ng
    "ALERT": 4,
    "WATCHLIST": 2
}

RSS_SOURCES = {"CoinDesk": "https://www.coindesk.com/arc/outboundfeeds/rss", "Cointelegraph": "https://cointelegraph.com/rss"}

# ==============================================================================
# HELPER & UTILITY FUNCTIONS (Kh√¥ng thay ƒë·ªïi)
# ==============================================================================
def load_json(file_path, default_value):
    if not os.path.exists(file_path): return default_value
    try:
        with open(file_path, 'r', encoding='utf-8') as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return default_value

def should_send_summary(last_summary_ts: float) -> bool:
    now_dt = datetime.now(VN_TZ)
    target_times = [(8, 2), (20, 2)]
    for hour, minute in target_times:
        target_dt_today = now_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
        if now_dt >= target_dt_today and last_summary_ts < target_dt_today.timestamp():
            return True
    return False

# ==============================================================================
# CORE ANALYSIS (N√ÇNG C·∫§P HO√ÄN TO√ÄN)
# ==============================================================================
def analyze_market_context_trend(mc: dict) -> str:
    if not mc: return "NEUTRAL"
    up_score, down_score = 0, 0
    btc_dom, fear_greed = mc.get('btc_dominance'), mc.get('fear_greed')
    if btc_dom is not None:
        if btc_dom > 55: up_score += 1
        elif btc_dom < 48: down_score += 1
    if fear_greed is not None:
        if fear_greed > 70: up_score += 1
        elif fear_greed < 30: down_score += 1
    if up_score > down_score: return "UPTREND"
    if down_score > up_score: return "DOWNTREND"
    return "NEUTRAL"

# *** N√ÇNG C·∫§P L·ªöN 3: H√ÄM PH√ÇN T√çCH V√Ä CH·∫§M ƒêI·ªÇM M·ªöI ***
def classify_and_score_news(title: str) -> Tuple[str, int]:
    """
    Ph√¢n t√≠ch ti√™u ƒë·ªÅ, t√≠nh t·ªïng ƒëi·ªÉm d·ª±a tr√™n KEYWORD_SCORES v√† tr·∫£ v·ªÅ
    (level, score).
    """
    title_l = title.lower()
    score = 0
    # D√πng set ƒë·ªÉ tr√°nh ƒë·∫øm 1 t·ª´ kh√≥a nhi·ªÅu l·∫ßn
    found_keywords = set()

    for keyword, value in KEYWORD_SCORES.items():
        # T√¨m ki·∫øm linh ho·∫°t h∆°n, kh√¥ng c·∫ßn ranh gi·ªõi t·ª´ \b
        if keyword in title_l and keyword not in found_keywords:
            score += value
            found_keywords.add(keyword)

    # Quy t·∫Øc ƒë·∫∑c bi·ªát cho tin C·∫£nh b√°o (Warning)
    if score <= -SCORE_THRESHOLDS["WARNING"]:
        return "WARNING", score

    # Ph√¢n lo·∫°i d·ª±a tr√™n ng∆∞·ª°ng ƒëi·ªÉm
    for level, threshold in SCORE_THRESHOLDS.items():
        if score >= threshold:
            return level, score

    return "INFO", score

def detect_category_tag(title: str) -> str:
    title_l = title.lower()
    # ∆Øu ti√™n t√¨m coin trong danh s√°ch theo d√µi
    for coin in WATCHED_COINS:
        # T√¨m ch√≠nh x√°c t√™n coin ƒë·ªÉ tr√°nh "link" kh·ªõp v·ªõi "Chainlink"
        if re.search(rf"\b{coin}\b", title_l):
            return coin.upper()

    # T√¨m c√°c m√£ token vi·∫øt hoa trong ngo·∫∑c ƒë∆°n ho·∫∑c ƒë·ª©ng ri√™ng l·∫ª
    match = re.search(r'\(([A-Z]{3,6})\)|\b([A-Z]{3,6})\b', title)
    if match:
        tag = (match.group(1) or match.group(2))
        if tag and tag.lower() not in ['USDT', 'CEO', 'CTO', 'TVL']:
             return tag.upper()

    # Ph√¢n lo·∫°i vƒ© m√¥ sau c√πng
    macro_keywords = ["fomc", "interest rate", "cpi", "inflation", "sec", "regulation", "fed", "market", "imf", "war"]
    if any(kw in title_l for kw in macro_keywords):
        return "MACRO"

    return "GENERAL"

def generate_specific_suggestion(score: int, market_trend: str) -> Tuple[str, str]:
    if score == 0:
        return "Tin t·ª©c tham kh·∫£o, t√°c ƒë·ªông kh√¥ng ƒë√°ng k·ªÉ ƒë·∫øn th·ªã tr∆∞·ªùng.", "‚ö™Ô∏è NEUTRAL"

    if score > 0: # Tin t√≠ch c·ª±c
        if market_trend == "UPTREND":
            return "T√°c ƒë·ªông T√çCH C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng tƒÉng. üëâ ∆Øu ti√™n c√°c l·ªánh MUA.", "üöÄ BULLISH"
        else:
            return "T√°c ƒë·ªông T√çCH C·ª∞C, c√≥ th·ªÉ t·∫°o s√≥ng h·ªìi ng·∫Øn. üëâ C√¢n nh·∫Øc l∆∞·ªõt s√≥ng.", "üìà POSITIVE"
    else: # Tin ti√™u c·ª±c
        if market_trend == "DOWNTREND":
            return "T√°c ƒë·ªông TI√äU C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng gi·∫£m. üëâ ∆Øu ti√™n c√°c l·ªánh B√ÅN.", "üìâ BEARISH"
        else:
            return "T√°c ƒë·ªông TI√äU C·ª∞C, c√≥ th·ªÉ g√¢y ƒëi·ªÅu ch·ªânh. üëâ Th·∫≠n tr·ªçng, c√¢n nh·∫Øc ch·ªët l·ªùi.", "üö® NEGATIVE"

def generate_final_summary(alerts: List[Dict], market_trend: str) -> str:
    if not alerts:
        return "Kh√¥ng c√≥ tin t·ª©c m·ªõi ƒë√°ng ch√∫ √Ω."
    level_counts = Counter(alert['level'] for alert in alerts)
    if level_counts['CRITICAL'] > 0:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ tin t·ª©c C·ª∞C K·ª≤ QUAN TR·ªåNG."
    elif level_counts['WARNING'] > 0:
        base_summary = "Xu·∫•t hi·ªán c√°c tin t·ª©c C·∫¢NH B√ÅO c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti√™u c·ª±c."
    elif level_counts['ALERT'] > 0:
        base_summary = "C√≥ nhi·ªÅu tin t·ª©c C·∫¨P NH·∫¨T v·ªÅ c√°c d·ª± √°n."
    else:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ m·ªôt v√†i tin t·ª©c m·ªõi."

    if market_trend == "UPTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang T√çCH C·ª∞C, c√°c tin t·ªët s·∫Ω ƒë∆∞·ª£c khu·∫øch ƒë·∫°i."
    elif market_trend == "DOWNTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang TI√äU C·ª∞C, c·∫ßn c·∫©n tr·ªçng v·ªõi c√°c tin x·∫•u."
    else:
        context_summary = "Th·ªã tr∆∞·ªùng chung ƒëang ƒëi ngang, gi√° s·∫Ω ph·∫£n ·ª©ng ch·ªß y·∫øu theo t·ª´ng tin ri√™ng l·∫ª."
    return f"‚ö†Ô∏è **ƒê√°nh gi√° chung:** {base_summary} {context_summary}"

# ==============================================================================
# DATA FETCHING & PERSISTENCE (Kh√¥ng thay ƒë·ªïi)
# ==============================================================================
def fetch_news_sources() -> List[Dict]:
    all_news = []
    try:
        url = "https://www.binance.com/bapi/composite/v1/public/cms/article/catalog/list/query?catalogId=48&pageSize=10&pageNo=1"
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        res.raise_for_status()
        articles = res.json().get("data", {}).get("articles", [])
        all_news.extend([{"id": f"binance_{item.get('code')}", "title": item.get("title"), "url": f"https://www.binance.com/en/support/announcement/{item.get('code')}", "source_name": "Binance"} for item in articles])
    except Exception as e: print(f"[ERROR] Binance fetch failed: {e}")
    for tag, url in RSS_SOURCES.items():
        try:
            feed = feedparser.parse(url)
            all_news.extend([{"id": f"{tag}_{hashlib.md5((entry.link + entry.title).encode()).hexdigest()[:12]}", "title": entry.title, "url": entry.link, "source_name": tag} for entry in feed.entries[:10]])
        except Exception as e: print(f"[ERROR] RSS {tag} fetch failed: {e}")
    return all_news

def save_news_for_precious(news_item: Dict):
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    if not any(item['id'] == news_item['id'] for item in logs):
        logs.insert(0, news_item)
        with open(fname, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

# ==============================================================================
# DISCORD & SUMMARY FUNCTIONS (C√ì THAY ƒê·ªîI NH·ªé)
# ==============================================================================
def send_discord_alert(message: str):
    if not DISCORD_WEBHOOK:
        print("[WARN] DISCORD_WEBHOOK is not set.")
        return
    max_len = 2000
    chunks = []
    if len(message) > max_len:
        lines = message.split('\n')
        current_chunk = ""
        for line in lines:
            if len(current_chunk) + len(line) + 1 > max_len:
                chunks.append(current_chunk)
                current_chunk = line
            else:
                current_chunk += ("\n" if current_chunk else "") + line
        chunks.append(current_chunk)
    else:
        chunks.append(message)
    for i, chunk in enumerate(chunks):
        if not chunk.strip(): continue
        try:
            content_to_send = f"(Ph·∫ßn {i+1}/{len(chunks)})\n{chunk}" if len(chunks) > 1 and i > 0 else chunk
            response = requests.post(DISCORD_WEBHOOK, json={"content": content_to_send}, timeout=10)
            response.raise_for_status()
            print(f"‚úÖ Sent chunk {i+1}/{len(chunks)}.")
            if len(chunks) > 1: time.sleep(1)
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] Failed to send chunk {i+1}: {e.response.text if e.response else e}")

def send_daily_summary():
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    if not logs:
        msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n_Kh√¥ng c√≥ tin t·ª©c t√≠n hi·ªáu n√†o ƒë∆∞·ª£c ghi nh·∫≠n trong ng√†y h√¥m nay._"
        send_discord_alert(msg)
        return
    context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)
    news_by_level = defaultdict(list)
    for item in logs:
        news_by_level[item['level']].append(item)
    msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n"
    msg += f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed', 'N/A')} | BTC.D: {context.get('btc_dominance', 'N/A')}% | Trend: {market_trend}```"
    level_order = ["CRITICAL", "WARNING", "ALERT", "WATCHLIST", "INFO"]
    for level in level_order:
        if level in news_by_level:
            emoji = {"CRITICAL": "üî¥", "WARNING": "‚ö†Ô∏è", "ALERT": "üì£", "WATCHLIST": "üëÄ", "INFO": "‚ÑπÔ∏è"}.get(level, "‚ÑπÔ∏è")
            # Th√™m ƒëi·ªÉm s·ªë v√†o ti√™u ƒë·ªÅ
            msg += f"\n{emoji} **{level}** ({len(news_by_level[level])} tin):\n"
            # S·∫Øp x·∫øp tin theo ƒë·ªô "hot" (gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa ƒëi·ªÉm)
            sorted_news = sorted(news_by_level[level], key=lambda x: abs(x.get('news_score', 0)), reverse=True)
            for item in sorted_news[:5]:
                score_str = f" (ƒêi·ªÉm: {item.get('news_score', 0)})"
                msg += f"- [{item['source_name']}] {item['title']}{score_str} [Link](<{item['url']}>)\n"
    send_discord_alert(msg)
    print("‚úÖ Daily Summary sent.")

# ==============================================================================
# MAIN EXECUTION (C·∫¨P NH·∫¨T THEO LOGIC M·ªöI)
# ==============================================================================
def main():
    print(f"--- Running News Cycle v6.0 (Scoring Engine) at {datetime.now(VN_TZ).strftime('%Y-%m-%d %H:%M:%S')} ---")

    cooldown_data = load_json(COOLDOWN_TRACKER, {"last_sent_id": [], "last_sent_level": {}, "last_summary": 0})
    now_ts = time.time()

    if should_send_summary(cooldown_data.get("last_summary", 0)):
        print("‚è∞ Time for Daily Summary...")
        send_daily_summary()
        cooldown_data["last_summary"] = now_ts

    try:
        context = get_market_context()
    except Exception as e:
        print(f"[ERROR] Failed to update market context: {e}")
        context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)

    all_news = fetch_news_sources()
    new_alerts_this_cycle = []
    
    for news in all_news:
        if news['id'] in cooldown_data.get("last_sent_id", []):
            continue
        
        level, score = classify_and_score_news(news['title'])
        suggestion, impact_tag = generate_specific_suggestion(score, market_trend)
        
        alert_item = {
            "id": news['id'], "title": news['title'], "url": news['url'],
            "source_name": news['source_name'], "published_at": news.get('published_at', datetime.now().isoformat()),
            "level": level,
            "news_score": score, # <-- TH√äM ƒêI·ªÇM S·ªê V√ÄO D·ªÆ LI·ªÜU
            "category_tag": detect_category_tag(news['title']),
            "suggestion": suggestion, "impact_tag": impact_tag
        }
        # Ch·ªâ x·ª≠ l√Ω c√°c tin c√≥ m·ª©c ƒë·ªô quan tr·ªçng t·ª´ WATCHLIST tr·ªü l√™n ho·∫∑c c√≥ ƒëi·ªÉm kh√°c 0
        if level != "INFO" or score != 0:
            new_alerts_this_cycle.append(alert_item)

    if not new_alerts_this_cycle:
        print("‚úÖ No new significant alerts to send for this cycle.")
    else:
        level_order = ["CRITICAL", "WARNING", "ALERT", "WATCHLIST", "INFO"]
        highest_level_in_news = "INFO"
        for level in level_order:
            if any(alert['level'] == level for alert in new_alerts_this_cycle):
                highest_level_in_news = level
                break
        
        cooldown_minutes = LEVEL_COOLDOWN_MINUTES.get(highest_level_in_news, 120)
        last_sent_time_for_level = cooldown_data.get("last_sent_level", {}).get(highest_level_in_news, 0)
        
        should_send_digest = True
        if now_ts - last_sent_time_for_level < cooldown_minutes * 60:
            print(f"‚è≥ Digest skipped. Highest level '{highest_level_in_news}' is on cooldown.")
            should_send_digest = False

        if should_send_digest:
            print(f"üî• Found {len(new_alerts_this_cycle)} new alerts. Highest level: {highest_level_in_news}. Sending digest...")
            
            for alert in new_alerts_this_cycle:
                save_news_for_precious(alert)
            print(f"‚úÖ Wrote/Updated {len(new_alerts_this_cycle)} items to signal file.")

            news_by_level = defaultdict(list)
            for alert in new_alerts_this_cycle:
                news_by_level[alert['level']].append(alert)

            context_block = f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed', 'N/A')} | BTC.D: {context.get('btc_dominance', 'N/A')}% | Trend: {market_trend}```"
            news_blocks = []
            for level in level_order:
                if level in news_by_level:
                    emoji = {"CRITICAL": "üî¥", "WARNING": "‚ö†Ô∏è", "ALERT": "üì£", "WATCHLIST": "üëÄ", "INFO": "‚ÑπÔ∏è"}.get(level, "‚ÑπÔ∏è")
                    level_header = f"{emoji} **{level}**"
                    news_blocks.append(level_header)
                    # S·∫Øp x·∫øp tin trong m·ªói level theo ƒëi·ªÉm s·ªë (cao -> th·∫•p)
                    sorted_alerts = sorted(news_by_level[level], key=lambda x: x['news_score'], reverse=True)
                    for alert in sorted_alerts:
                        score_str = f" (ƒêi·ªÉm: {alert.get('news_score', 0)})"
                        part = (f"- **[{alert['source_name']}] {alert['title']}**\n"
                                f"  *‚Ü≥ Nh·∫≠n ƒë·ªãnh:* {alert['suggestion']}{score_str} [Link](<{alert['url']}>)")
                        news_blocks.append(part)

            final_summary = generate_final_summary(new_alerts_this_cycle, market_trend)
            full_digest_message = (f"**üî• B·∫¢N TIN TH·ªä TR∆Ø·ªúNG - {datetime.now(VN_TZ).strftime('%H:%M')} üî•**\n\n"
                                   + context_block + "\n\n"
                                   + "\n".join(news_blocks)
                                   + f"\n\n{final_summary}")
            send_discord_alert(full_digest_message)

            sent_ids_this_cycle = [alert['id'] for alert in new_alerts_this_cycle]
            cooldown_data["last_sent_id"] = (cooldown_data.get("last_sent_id", []) + sent_ids_this_cycle)[-50:]
            
            updated_levels = set(alert['level'] for alert in new_alerts_this_cycle)
            for level in updated_levels:
                cooldown_data.get("last_sent_level", {})[level] = now_ts
    
    with open(COOLDOWN_TRACKER, 'w') as f:
        json.dump(cooldown_data, f, indent=2)

    print("--- Cycle Finished ---")

if __name__ == "__main__":
    main()
