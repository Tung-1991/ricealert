# -*- coding: utf-8 -*-
# rice_news.py
# Version: 7.0 (Final Hybrid - Full Code)
# Description: Phi√™n b·∫£n ho√†n thi·ªán, s·ª≠ d·ª•ng Scoring Engine l√†m b·ªô l·ªçc ch√≠nh
#              v√† g·ªçi API Google Gemini (gemini-2.5-flash-lite) ƒë·ªÉ ph√¢n t√≠ch s√¢u
#              c√°c tin t·ª©c quan tr·ªçng. T√≠ch h·ª£p c∆° ch·∫ø fallback an to√†n.

import os
import json
import requests
import feedparser
import hashlib
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import time
import re
from typing import List, Dict, Tuple, Optional
from collections import defaultdict, Counter

# Import market_context functions
# Gi·∫£ ƒë·ªãnh file market_context.py n·∫±m ·ªü th∆∞ m·ª•c cha (/root/ricealert/)
# ƒê·ªÉ import ƒë√∫ng, ch√∫ng ta c·∫ßn th√™m th∆∞ m·ª•c cha v√†o sys.path
import sys
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
from market_context import get_market_context_data, get_market_context

# T√≠ch h·ª£p th∆∞ vi·ªán Google AI
try:
    import google.generativeai as genai
except ImportError:
    print("[ERROR] 'google-generativeai' library not found. Please install it using: pip install google-generativeai")
    genai = None

# ==============================================================================
# CONFIG & SETUP
# ==============================================================================
# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env trong th∆∞ m·ª•c cha c·ªßa d·ª± √°n
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
load_dotenv(dotenv_path=dotenv_path)

VN_TZ = timezone(timedelta(hours=7))
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_DIR = os.path.join(BASE_DIR, "lognew")
# ƒê·∫∑t file cooldown ·ªü th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n ƒë·ªÉ d·ªÖ qu·∫£n l√Ω
COOLDOWN_TRACKER = os.path.join(os.path.dirname(BASE_DIR), "cooldown_tracker.json")
DISCORD_WEBHOOK = os.getenv("DISCORD_NEWS_WEBHOOK")
os.makedirs(LOG_DIR, exist_ok=True)

# C·∫•u h√¨nh cho Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY and genai:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    print("[WARN] GOOGLE_API_KEY not found or 'google-generativeai' library missing. AI analysis will be skipped.")

# Ng∆∞·ª°ng ƒëi·ªÉm t·ª´ kh√≥a ƒë·ªÉ k√≠ch ho·∫°t ph√¢n t√≠ch c·ªßa AI
AI_ANALYSIS_THRESHOLD = 7.0

# C√°c c·∫•u h√¨nh kh√°c
WATCHED_COINS = set([s.replace("USDT", "").lower() for s in os.getenv("SYMBOLS", "").split(",")])
WATCHED_COINS.update(["btc", "eth", "xrp", "sol", "bnb"])
LEVEL_COOLDOWN_MINUTES = { "CRITICAL": 15, "WARNING": 45, "ALERT": 90, "WATCHLIST": 180, "INFO": 360 }
SUMMARY_COOLDOWN_SECONDS = 6 * 60 * 60
KEYWORD_SCORES = {
    "etf approval": 10, "etf approved": 10, "regulatory approval": 10, "will list": 8, "listing": 7, "halving": 8, "fomc": 7,
    "interest rate": 7, "cpi": 7, "war": -9, "sec sues": -9, "sec charges": -9, "hack": -10, "exploit": -10, "lawsuit": -8,
    "delist": -8, "b·ªã ƒëi·ªÅu tra": -9, "ki·ªán": -8, "downtime": -7, "outage": -7, "unlock": -6, "partnership": 5, "mainnet": 6,
    "upgrade": 5, "launch": 5, "adoption": 5, "margin": 4, "futures": 4, "available on": 4, "will add": 4,
    "maintenance": -5, "regulation": -6, "testnet": 3, "airdrop": 3, "voting": 2, "ama": 1, "token burn": 4,
    "governance": 2, "tvl hits record": 5, "record": 2, "huge": 2, "massive": 2, "significant": 2, "major": 2,
    "minor": -1, "delay": -3, "vulnerability": -7
}
SCORE_THRESHOLDS = { "CRITICAL": 8, "WARNING": 5, "ALERT": 4, "WATCHLIST": 2 }
RSS_SOURCES = {"CoinDesk": "https://www.coindesk.com/arc/outboundfeeds/rss", "Cointelegraph": "https://cointelegraph.com/rss"}

# ==============================================================================
# CORE & HELPER FUNCTIONS
# ==============================================================================

def get_ai_analysis(title: str) -> Optional[str]:
    """
    G·ª≠i ti√™u ƒë·ªÅ ƒë·∫øn Gemini ƒë·ªÉ ph√¢n t√≠ch.
    N·∫øu th√†nh c√¥ng, tr·∫£ v·ªÅ chu·ªói ph√¢n t√≠ch.
    N·∫øu th·∫•t b·∫°i ho·∫∑c kh√¥ng c√≥ key, tr·∫£ v·ªÅ None.
    """
    if not GOOGLE_API_KEY or not genai:
        return None
        
    try:
        model = genai.GenerativeModel('gemini-2.5-flash-lite')
        
        prompt = f"""As a crypto trading analyst, what is the potential market impact of this headline? 
        Is it bullish, bearish, or neutral, and why? Be very concise, one short Vietnamese sentence only.
        Headline: "{title}"
        Analysis:"""
        
        response = model.generate_content(prompt, request_options={'timeout': 20})
        return response.text.strip()
    except Exception as e:
        print(f"[WARN] AI analysis call failed: {e}")
        return None # Fallback an to√†n

def load_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return default

def should_send_summary(last_summary_ts: float) -> bool:
    now_dt = datetime.now(VN_TZ)
    target_times = [(8, 2), (20, 2)]
    for hour, minute in target_times:
        target_dt_today = now_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
        if now_dt >= target_dt_today and last_summary_ts < target_dt_today.timestamp():
            return True
    return False

def analyze_market_context_trend(mc: dict) -> str:
    if not mc: return "NEUTRAL"
    up_score, down_score = 0, 0
    btc_dom, fear_greed = mc.get('btc_dominance'), mc.get('fear_greed')
    if btc_dom is not None:
        if btc_dom > 55: up_score += 1
        elif btc_dom < 48: down_score += 1
    if fear_greed is not None:
        if fear_greed > 70: up_score += 1
        elif fear_greed < 30: down_score += 1
    if up_score > down_score: return "UPTREND"
    if down_score > up_score: return "DOWNTREND"
    return "NEUTRAL"

def classify_and_score_news(title: str) -> Tuple[str, int]:
    title_l = title.lower(); score = 0; found_keywords = set()
    for keyword, value in KEYWORD_SCORES.items():
        if keyword in title_l and keyword not in found_keywords:
            score += value; found_keywords.add(keyword)
    if score <= -SCORE_THRESHOLDS["WARNING"]: return "WARNING", score
    for level, threshold in SCORE_THRESHOLDS.items():
        if score >= threshold: return level, score
    return "INFO", score

def detect_category_tag(title: str) -> str:
    title_l = title.lower()
    for coin in WATCHED_COINS:
        if re.search(rf"\b{coin}\b", title_l):
            return coin.upper()
    match = re.search(r'\(([A-Z]{3,6})\)|\b([A-Z]{3,6})\b', title)
    if match:
        tag = (match.group(1) or match.group(2))
        if tag and tag.lower() not in ['USDT', 'CEO', 'CTO', 'TVL']:
             return tag.upper()
    macro_keywords = ["fomc", "interest rate", "cpi", "inflation", "sec", "regulation", "fed", "market", "imf", "war"]
    if any(kw in title_l for kw in macro_keywords):
        return "MACRO"
    return "GENERAL"

def generate_specific_suggestion(score: int, market_trend: str) -> Tuple[str, str]:
    if score == 0: return "Tin t·ª©c tham kh·∫£o, t√°c ƒë·ªông kh√¥ng ƒë√°ng k·ªÉ ƒë·∫øn th·ªã tr∆∞·ªùng.", "‚ö™Ô∏è NEUTRAL"
    if score > 0:
        if market_trend == "UPTREND": return "T√°c ƒë·ªông T√çCH C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng tƒÉng. üëâ ∆Øu ti√™n c√°c l·ªánh MUA.", "üöÄ BULLISH"
        else: return "T√°c ƒë·ªông T√çCH C·ª∞C, c√≥ th·ªÉ t·∫°o s√≥ng h·ªìi ng·∫Øn. üëâ C√¢n nh·∫Øc l∆∞·ªõt s√≥ng.", "üìà POSITIVE"
    else:
        if market_trend == "DOWNTREND": return "T√°c ƒë·ªông TI√äU C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng gi·∫£m. üëâ ∆Øu ti√™n c√°c l·ªánh B√ÅN.", "üìâ BEARISH"
        else: return "T√°c ƒë·ªông TI√äU C·ª∞C, c√≥ th·ªÉ g√¢y ƒëi·ªÅu ch·ªânh. üëâ Th·∫≠n tr·ªçng, c√¢n nh·∫Øc ch·ªët l·ªùi.", "üö® NEGATIVE"

def generate_final_summary(alerts: List[Dict], market_trend: str) -> str:
    if not alerts:
        return "Kh√¥ng c√≥ tin t·ª©c m·ªõi ƒë√°ng ch√∫ √Ω."
    level_counts = Counter(alert['level'] for alert in alerts)
    if level_counts['CRITICAL'] > 0:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ tin t·ª©c C·ª∞C K·ª≤ QUAN TR·ªåNG."
    elif level_counts['WARNING'] > 0:
        base_summary = "Xu·∫•t hi·ªán c√°c tin t·ª©c C·∫¢NH B√ÅO c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ti√™u c·ª±c."
    elif level_counts['ALERT'] > 0:
        base_summary = "C√≥ nhi·ªÅu tin t·ª©c C·∫¨P NH·∫¨T v·ªÅ c√°c d·ª± √°n."
    else:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ m·ªôt v√†i tin t·ª©c m·ªõi."

    if market_trend == "UPTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang T√çCH C·ª∞C, c√°c tin t·ªët s·∫Ω ƒë∆∞·ª£c khu·∫øch ƒë·∫°i."
    elif market_trend == "DOWNTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang TI√äU C·ª∞C, c·∫ßn c·∫©n tr·ªçng v·ªõi c√°c tin x·∫•u."
    else:
        context_summary = "Th·ªã tr∆∞·ªùng chung ƒëang ƒëi ngang, gi√° s·∫Ω ph·∫£n ·ª©ng ch·ªß y·∫øu theo t·ª´ng tin ri√™ng l·∫ª."
    return f"‚ö†Ô∏è **ƒê√°nh gi√° chung:** {base_summary} {context_summary}"

def fetch_news_sources() -> List[Dict]:
    all_news = []; headers = {"User-Agent": "Mozilla/5.0"}
    try:
        url="https://www.binance.com/bapi/composite/v1/public/cms/article/catalog/list/query?catalogId=48&pageSize=20&pageNo=1"
        res=requests.get(url, headers=headers, timeout=10); res.raise_for_status()
        articles=res.json().get("data",{}).get("articles",[])
        all_news.extend([{"id":f"binance_{item.get('code')}","title":item.get("title"),"url":f"https://www.binance.com/en/support/announcement/{item.get('code')}","source_name":"Binance"} for item in articles])
    except Exception as e: print(f"[ERROR] Binance fetch failed: {e}")
    for tag, url in RSS_SOURCES.items():
        try:
            feed=feedparser.parse(url, request_headers=headers)
            all_news.extend([{"id":f"{tag}_{hashlib.md5((entry.link+entry.title).encode()).hexdigest()[:12]}","title":entry.title,"url":entry.link,"source_name":tag} for entry in feed.entries[:20]])
        except Exception as e: print(f"[ERROR] RSS {tag} fetch failed: {e}")
    return all_news

def save_news_for_precious(news_item: Dict):
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    # Tr√°nh l∆∞u tr√πng l·∫∑p
    if not any(item['id'] == news_item['id'] for item in logs):
        logs.insert(0, news_item)
        # Gi·ªõi h·∫°n file log ·ªü 200 tin ƒë·ªÉ tr√°nh qu√° l·ªõn
        with open(fname, 'w', encoding='utf-8') as f:
            json.dump(logs[:200], f, indent=2, ensure_ascii=False)

def send_discord_alert(message: str):
    if not DISCORD_WEBHOOK:
        print("[WARN] DISCORD_WEBHOOK is not set.")
        return
    max_len=2000; chunks=[];
    if len(message)>max_len:
        lines=message.split('\n'); current_chunk=""
        for line in lines:
            if len(current_chunk)+len(line)+1>max_len:
                chunks.append(current_chunk)
                current_chunk = line
            else:
                current_chunk+=("\n" if current_chunk else "")+line
        chunks.append(current_chunk)
    else:
        chunks.append(message)
    for i,chunk in enumerate(chunks):
        if not chunk.strip(): continue
        try:
            content_to_send=f"(Ph·∫ßn {i+1}/{len(chunks)})\n{chunk}" if len(chunks)>1 and i>0 else chunk
            response=requests.post(DISCORD_WEBHOOK,json={"content":content_to_send},timeout=10)
            response.raise_for_status()
            print(f"‚úÖ Sent chunk {i+1}/{len(chunks)}.")
            if len(chunks)>1: time.sleep(1)
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] Failed to send chunk {i+1}: {e.response.text if e.response else e}")

def send_daily_summary():
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    if not logs:
        msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n_Kh√¥ng c√≥ tin t·ª©c t√≠n hi·ªáu n√†o ƒë∆∞·ª£c ghi nh·∫≠n trong ng√†y h√¥m nay._"
        send_discord_alert(msg)
        return
    context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)
    news_by_level = defaultdict(list)
    for item in logs:
        news_by_level[item['level']].append(item)
    msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n"
    msg += f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed', 'N/A')} | BTC.D: {context.get('btc_dominance', 'N/A')}% | Trend: {market_trend}```"
    level_order = ["CRITICAL", "WARNING", "ALERT", "WATCHLIST", "INFO"]
    for level in level_order:
        if level in news_by_level:
            emoji = {"CRITICAL": "üî¥", "WARNING": "‚ö†Ô∏è", "ALERT": "üì£", "WATCHLIST": "üëÄ", "INFO": "‚ÑπÔ∏è"}.get(level, "‚ÑπÔ∏è")
            msg += f"\n{emoji} **{level}** ({len(news_by_level[level])} tin):\n"
            sorted_news = sorted(news_by_level[level], key=lambda x: abs(x.get('news_score', 0)), reverse=True)
            for item in sorted_news[:5]:
                score_str = f" (ƒêi·ªÉm: {item.get('news_score', 0)})"
                msg += f"- [{item['source_name']}] {item['title']}{score_str} [Link](<{item['url']}>)\n"
    send_discord_alert(msg)
    print("‚úÖ Daily Summary sent.")

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
def main():
    print(f"--- Running News Cycle v7.0 (Final Hybrid) at {datetime.now(VN_TZ).strftime('%Y-%m-%d %H:%M:%S')} ---")
    
    cooldown_data = load_json(COOLDOWN_TRACKER, {"last_sent_id": [], "last_sent_level": {}, "last_summary": 0})
    now_ts = time.time()

    if should_send_summary(cooldown_data.get("last_summary", 0)):
        print("‚è∞ Time for Daily Summary...")
        send_daily_summary()
        cooldown_data["last_summary"] = now_ts
    
    try:
        context = get_market_context()
    except Exception as e:
        print(f"[ERROR] Failed to update market context: {e}")
        context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)

    all_news = fetch_news_sources()
    new_alerts_this_cycle = []
    
    for news in all_news:
        if news['id'] in cooldown_data.get("last_sent_id", []):
            continue

        level, score = classify_and_score_news(news['title'])
        suggestion, impact_tag = generate_specific_suggestion(score, market_trend)
        
        # LOGIC HYBRID N·∫∞M ·ªû ƒê√ÇY
        if abs(score) >= AI_ANALYSIS_THRESHOLD:
            print(f"üî• High-score news found (Score: {score}). Triggering AI analysis for: \"{news['title']}\"")
            ai_suggestion = get_ai_analysis(news['title'])
            if ai_suggestion: # N·∫øu AI tr·∫£ v·ªÅ k·∫øt qu·∫£ (kh√¥ng l·ªói)
                suggestion += f"\n  üß† **AI Ph√¢n t√≠ch s√¢u:** *{ai_suggestion}*"

        alert_item = {
            "id": news['id'], "title": news['title'], "url": news['url'],
            "source_name": news['source_name'], "published_at": datetime.now().isoformat(),
            "level": level, "news_score": score,
            "category_tag": detect_category_tag(news['title']),
            "suggestion": suggestion, # Suggestion n√†y gi·ªù c√≥ th·ªÉ ch·ª©a c·∫£ ph√¢n t√≠ch c·ªßa AI
            "impact_tag": impact_tag
        }
        
        if level != "INFO" or score != 0:
            new_alerts_this_cycle.append(alert_item)

    if not new_alerts_this_cycle:
        print("‚úÖ No new significant alerts to send for this cycle.")
        return

    # Logic g·ª≠i tin nh·∫Øn
    level_order=["CRITICAL","WARNING","ALERT","WATCHLIST","INFO"]
    highest_level_in_news="INFO"
    for level in level_order:
        if any(alert['level']==level for alert in new_alerts_this_cycle):
            highest_level_in_news=level
            break
    
    cooldown_minutes=LEVEL_COOLDOWN_MINUTES.get(highest_level_in_news,120)
    last_sent_time_for_level=cooldown_data.get("last_sent_level",{}).get(highest_level_in_news,0)
    
    if time.time() - last_sent_time_for_level > cooldown_minutes * 60:
        print(f"üî• Found {len(new_alerts_this_cycle)} new alerts. Highest level: {highest_level_in_news}. Sending digest...")
        
        # X√¢y d·ª±ng v√† g·ª≠i tin nh·∫Øn Discord
        news_by_level=defaultdict(list)
        for alert in new_alerts_this_cycle:
            news_by_level[alert['level']].append(alert)
        
        context_block=f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed','N/A')} | BTC.D: {context.get('btc_dominance','N/A')}% | Trend: {market_trend}```"
        news_blocks=[]
        for level in level_order:
            if level in news_by_level:
                emoji={"CRITICAL":"üî¥","WARNING":"‚ö†Ô∏è","ALERT":"üì£","WATCHLIST":"üëÄ","INFO":"‚ÑπÔ∏è"}.get(level,"‚ÑπÔ∏è")
                level_header=f"{emoji} **{level}**"
                news_blocks.append(level_header)
                sorted_alerts=sorted(news_by_level[level],key=lambda x:abs(x.get('news_score', 0)), reverse=True)
                for alert in sorted_alerts:
                    part = (f"- **[{alert['source_name']}] {alert['title']}**\n"
                            f"  *‚Ü≥ Nh·∫≠n ƒë·ªãnh:* {alert['suggestion']} [Link](<{alert['url']}>)")
                    news_blocks.append(part)
        
        final_summary=generate_final_summary(new_alerts_this_cycle, market_trend)
        full_digest_message=(f"**üî• B·∫¢N TIN TH·ªä TR∆Ø·ªúNG - {datetime.now(VN_TZ).strftime('%H:%M')} üî•**\n\n"+context_block+"\n\n"+"\n".join(news_blocks)+f"\n\n{final_summary}")
        
        send_discord_alert(full_digest_message)
        
        # C·∫≠p nh·∫≠t tr·∫°ng th√°i cooldown
        sent_ids_this_cycle=[alert['id'] for alert in new_alerts_this_cycle]
        cooldown_data["last_sent_id"]=(cooldown_data.get("last_sent_id",[])+sent_ids_this_cycle)[-50:]
        cooldown_data.get("last_sent_level",{})[highest_level_in_news] = time.time()
        
        with open(COOLDOWN_TRACKER, 'w') as f:
            json.dump(cooldown_data, f, indent=2)
    else:
        print(f"‚è≥ Digest skipped. Highest level '{highest_level_in_news}' is on cooldown.")

    print("--- Cycle Finished ---")

if __name__ == "__main__":
    main()
