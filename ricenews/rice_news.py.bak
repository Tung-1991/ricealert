# -*- coding: utf-8 -*-
# rice_news_final.py
# Version: 7.0 (Logical & Stable)

import os
import json
import requests
import feedparser
import hashlib
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import time
import re
from typing import List, Dict, Tuple
from collections import defaultdict, Counter

try:
    from market_context import get_market_context_data, get_market_context
except ImportError:
    def get_market_context_data(): return {}
    def get_market_context(): return {}

# ==============================================================================
# CONFIG & SETUP
# ==============================================================================
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
load_dotenv(dotenv_path=dotenv_path)

VN_TZ = timezone(timedelta(hours=7))
BASE_DIR = os.getenv("RICENEWS_BASE_DIR", "/root/ricealert/ricenews")
LOG_DIR = os.path.join(BASE_DIR, "lognew")
COOLDOWN_TRACKER = os.path.join(BASE_DIR, "cooldown_tracker.json")
DISCORD_WEBHOOK = os.getenv("DISCORD_NEWS_WEBHOOK")
os.makedirs(LOG_DIR, exist_ok=True)

WATCHED_COINS = set([s.replace("USDT", "").lower() for s in os.getenv("SYMBOLS", "").split(",")])
WATCHED_COINS.update(["btc", "eth", "xrp", "sol", "bnb"])

LEVEL_COOLDOWN_MINUTES = {
    "CRITICAL": 90, "WARNING": 180, "ALERT": 240,
    "WATCHLIST": 300, "INFO": 350
}
SUMMARY_COOLDOWN_SECONDS = 6 * 60 * 60

KEYWORD_SCORES = {
    "etf approval": 10, "etf approved": 10, "regulatory approval": 10, "will list": 8, "listing": 7,
    "halving": 8, "fomc": 7, "interest rate": 7, "cpi": 7, "war": -9, "sec sues": -9, "sec charges": -9,
    "hack": -10, "exploit": -10, "lawsuit": -8, "delist": -8, "b·ªã ƒëi·ªÅu tra": -9, "ki·ªán": -8, "downtime": -7, "outage": -7,
    "unlock": -6, "partnership": 5, "mainnet": 6, "upgrade": 5, "launch": 5, "adoption": 5,
    "margin": 4, "futures": 4, "available on": 4, "will add": 4,
    "maintenance": -5, "regulation": -6, "testnet": 3, "airdrop": 3, "voting": 2, "ama": 1, "token burn": 4, "governance": 2, "tvl hits record": 5,
    "record": 2, "huge": 2, "massive": 2, "significant": 2,
    "major": 2, "minor": -1, "delay": -3, "vulnerability": -7
}

SCORE_THRESHOLDS = {
    "CRITICAL": 8,
    "WARNING": 5,
    "ALERT": 4,
    "WATCHLIST": 2
}

RSS_SOURCES = {"CoinDesk": "https://www.coindesk.com/arc/outboundfeeds/rss", "Cointelegraph": "https://cointelegraph.com/rss"}

# ==============================================================================
# HELPER & UTILITY FUNCTIONS
# ==============================================================================
def load_json(file_path, default_value):
    if not os.path.exists(file_path): return default_value
    try:
        with open(file_path, 'r', encoding='utf-8') as f: return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError): return default_value

def should_send_summary(last_summary_ts: float) -> bool:
    now_dt = datetime.now(VN_TZ)
    target_times = [(8, 2), (20, 2)]
    for hour, minute in target_times:
        target_dt_today = now_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
        if now_dt >= target_dt_today and last_summary_ts < target_dt_today.timestamp():
            return True
    return False

# ==============================================================================
# CORE ANALYSIS
# ==============================================================================
def analyze_market_context_trend(mc: dict) -> str:
    if not mc: return "NEUTRAL"
    up_score, down_score = 0, 0
    btc_dom, fear_greed = mc.get('btc_dominance'), mc.get('fear_greed')
    if btc_dom is not None:
        if btc_dom > 55: up_score += 1
        elif btc_dom < 48: down_score += 1
    if fear_greed is not None:
        if fear_greed > 70: up_score += 1
        elif fear_greed < 30: down_score += 1
    if up_score > down_score: return "UPTREND"
    if down_score > up_score: return "DOWNTREND"
    return "NEUTRAL"

def classify_and_score_news(title: str) -> Tuple[str, int]:
    title_l = title.lower()
    score = 0
    found_keywords = set()
    for keyword, value in KEYWORD_SCORES.items():
        if keyword in title_l and keyword not in found_keywords:
            score += value
            found_keywords.add(keyword)

    abs_score = abs(score)

    for level, threshold in SCORE_THRESHOLDS.items():
        if abs_score >= threshold:
            return level, score

    return "INFO", score

def detect_category_tag(title: str) -> str:
    title_l = title.lower()
    for coin in WATCHED_COINS:
        if re.search(rf"\b{coin}\b", title_l):
            return coin.upper()
    match = re.search(r'\(([A-Z]{3,6})\)|\b([A-Z]{3,6})\b', title)
    if match:
        tag = (match.group(1) or match.group(2))
        if tag and tag.lower() not in ['USDT', 'CEO', 'CTO', 'TVL']:
            return tag.upper()
    macro_keywords = ["fomc", "interest rate", "cpi", "inflation", "sec", "regulation", "fed", "market", "imf", "war"]
    if any(kw in title_l for kw in macro_keywords):
        return "MACRO"
    return "GENERAL"

def generate_specific_suggestion(score: int, market_trend: str) -> Tuple[str, str]:
    if score == 0:
        return "Tin t·ª©c tham kh·∫£o, t√°c ƒë·ªông kh√¥ng ƒë√°ng k·ªÉ ƒë·∫øn th·ªã tr∆∞·ªùng.", "‚ö™Ô∏è NEUTRAL"
    if score > 0:
        if market_trend == "UPTREND":
            return "T√°c ƒë·ªông T√çCH C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng tƒÉng. üëâ ∆Øu ti√™n c√°c l·ªánh MUA.", "üöÄ BULLISH"
        else:
            return "T√°c ƒë·ªông T√çCH C·ª∞C, c√≥ th·ªÉ t·∫°o s√≥ng h·ªìi ng·∫Øn. üëâ C√¢n nh·∫Øc l∆∞·ªõt s√≥ng.", "üìà POSITIVE"
    else:
        if market_trend == "DOWNTREND":
            return "T√°c ƒë·ªông TI√äU C·ª∞C, c·ªßng c·ªë xu h∆∞·ªõng gi·∫£m. üëâ ∆Øu ti√™n c√°c l·ªánh B√ÅN.", "üìâ BEARISH"
        else:
            return "T√°c ƒë·ªông TI√äU C·ª∞C, c√≥ th·ªÉ g√¢y ƒëi·ªÅu ch·ªânh. üëâ Th·∫≠n tr·ªçng, c√¢n nh·∫Øc ch·ªët l·ªùi.", "üö® NEGATIVE"

def generate_final_summary(alerts: List[Dict], market_trend: str) -> str:
    if not alerts:
        return "Kh√¥ng c√≥ tin t·ª©c m·ªõi ƒë√°ng ch√∫ √Ω."
    level_counts = Counter(alert['level'] for alert in alerts)
    if level_counts['CRITICAL'] > 0:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ tin t·ª©c C·ª∞C K·ª≤ QUAN TR·ªåNG."
    elif level_counts['WARNING'] > 0:
        base_summary = "Xu·∫•t hi·ªán c√°c tin t·ª©c C·∫¢NH B√ÅO c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn th·ªã tr∆∞·ªùng."
    elif level_counts['ALERT'] > 0:
        base_summary = "C√≥ nhi·ªÅu tin t·ª©c C·∫¨P NH·∫¨T v·ªÅ c√°c d·ª± √°n."
    else:
        base_summary = "Th·ªã tr∆∞·ªùng c√≥ m·ªôt v√†i tin t·ª©c m·ªõi."

    if market_trend == "UPTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang T√çCH C·ª∞C, c√°c tin t·ªët s·∫Ω ƒë∆∞·ª£c khu·∫øch ƒë·∫°i."
    elif market_trend == "DOWNTREND":
        context_summary = "B·ªëi c·∫£nh th·ªã tr∆∞·ªùng chung ƒëang TI√äU C·ª∞C, c·∫ßn c·∫©n tr·ªçng v·ªõi c√°c tin x·∫•u."
    else:
        context_summary = "Th·ªã tr∆∞·ªùng chung ƒëang ƒëi ngang, gi√° s·∫Ω ph·∫£n ·ª©ng ch·ªß y·∫øu theo t·ª´ng tin ri√™ng l·∫ª."
    return f"‚ö†Ô∏è **ƒê√°nh gi√° chung:** {base_summary} {context_summary}"

# ==============================================================================
# DATA FETCHING & PERSISTENCE
# ==============================================================================
def fetch_news_sources() -> List[Dict]:
    all_news = []
    try:
        url = "https://www.binance.com/bapi/composite/v1/public/cms/article/catalog/list/query?catalogId=48&pageSize=10&pageNo=1"
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        res.raise_for_status()
        articles = res.json().get("data", {}).get("articles", [])
        all_news.extend([{"id": f"binance_{item.get('code')}", "title": item.get("title"), "url": f"https://www.binance.com/en/support/announcement/{item.get('code')}", "source_name": "Binance"} for item in articles])
    except Exception as e: print(f"[ERROR] Binance fetch failed: {e}")
    for tag, url in RSS_SOURCES.items():
        try:
            feed = feedparser.parse(url)
            all_news.extend([{"id": f"{tag}_{hashlib.md5((entry.link + entry.title).encode()).hexdigest()[:12]}", "title": entry.title, "url": entry.link, "source_name": tag} for entry in feed.entries[:10]])
        except Exception as e: print(f"[ERROR] RSS {tag} fetch failed: {e}")
    return all_news

def save_news_for_precious(news_item: Dict):
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    if not any(item['id'] == news_item['id'] for item in logs):
        logs.insert(0, news_item)
        with open(fname, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

# ==============================================================================
# DISCORD & SUMMARY FUNCTIONS
# ==============================================================================
def send_discord_alert(message: str):
    if not DISCORD_WEBHOOK:
        print("[WARN] DISCORD_WEBHOOK is not set.")
        return
    max_len = 2000
    chunks = []
    if len(message) > max_len:
        lines = message.split('\n')
        current_chunk = ""
        for line in lines:
            if len(current_chunk) + len(line) + 1 > max_len:
                chunks.append(current_chunk)
                current_chunk = line
            else:
                current_chunk += ("\n" if current_chunk else "") + line
        chunks.append(current_chunk)
    else:
        chunks.append(message)
    for i, chunk in enumerate(chunks):
        if not chunk.strip(): continue
        try:
            content_to_send = f"(Ph·∫ßn {i+1}/{len(chunks)})\n{chunk}" if len(chunks) > 1 and i > 0 else chunk
            response = requests.post(DISCORD_WEBHOOK, json={"content": content_to_send}, timeout=10)
            response.raise_for_status()
            print(f"‚úÖ Sent chunk {i+1}/{len(chunks)}.")
            if len(chunks) > 1: time.sleep(1)
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] Failed to send chunk {i+1}: {e.response.text if e.response else e}")

def send_daily_summary():
    fname = os.path.join(LOG_DIR, f"{datetime.now(VN_TZ).strftime('%Y-%m-%d')}_news_signal.json")
    logs = load_json(fname, [])
    if not logs:
        msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n_Kh√¥ng c√≥ tin t·ª©c t√≠n hi·ªáu n√†o ƒë∆∞·ª£c ghi nh·∫≠n trong ng√†y h√¥m nay._"
        send_discord_alert(msg)
        return
    context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)
    news_by_level = defaultdict(list)
    for item in logs:
        news_by_level[item['level']].append(item)
    msg = f"**üìä B·∫¢N TIN T·ªîNG QUAN - {datetime.now(VN_TZ).strftime('%H:%M %d/%m')}**\n\n"
    msg += f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed', 'N/A')} | BTC.D: {context.get('btc_dominance', 'N/A')}% | Trend: {market_trend}```"
    level_order = ["CRITICAL", "WARNING", "ALERT", "WATCHLIST", "INFO"]
    for level in level_order:
        if level in news_by_level:
            emoji = {"CRITICAL": "üî¥", "WARNING": "‚ö†Ô∏è", "ALERT": "üì£", "WATCHLIST": "üëÄ", "INFO": "‚ÑπÔ∏è"}.get(level, "‚ÑπÔ∏è")
            msg += f"\n{emoji} **{level}** ({len(news_by_level[level])} tin):\n"
            sorted_news = sorted(news_by_level[level], key=lambda x: abs(x.get('news_score', 0)), reverse=True)
            for item in sorted_news[:5]:
                score_str = f" (ƒêi·ªÉm: {item.get('news_score', 0)})"
                msg += f"- [{item['source_name']}] {item['title']}{score_str} [Link](<{item['url']}>)\n"
    send_discord_alert(msg)
    print("‚úÖ Daily Summary sent.")

# ==============================================================================
# MAIN EXECUTION
# ==============================================================================
def main():
    print(f"--- Running News Cycle v7.0 (Logical) at {datetime.now(VN_TZ).strftime('%Y-%m-%d %H:%M:%S')} ---")
    cooldown_data = load_json(COOLDOWN_TRACKER, {"last_sent_id": [], "last_sent_level": {}, "last_summary": 0})
    now_ts = time.time()

    if should_send_summary(cooldown_data.get("last_summary", 0)):
        print("‚è∞ Time for Daily Summary...")
        send_daily_summary()
        cooldown_data["last_summary"] = now_ts

    try:
        context = get_market_context()
    except Exception as e:
        context = get_market_context_data()
    market_trend = analyze_market_context_trend(context)

    all_news = fetch_news_sources()
    new_alerts_this_cycle = []

    for news in all_news:
        if news['id'] in cooldown_data.get("last_sent_id", []):
            continue

        level, score = classify_and_score_news(news['title'])
        suggestion, _ = generate_specific_suggestion(score, market_trend)

        alert_item = {
            "id": news['id'],
            "title": news['title'],
            "url": news['url'],
            "source_name": news['source_name'],
            "published_at": datetime.now(VN_TZ).isoformat(),
            "level": level,
            "news_score": score,
            "category_tag": detect_category_tag(news['title']),
            "suggestion": suggestion,
        }
        if level != "INFO" or score != 0:
            new_alerts_this_cycle.append(alert_item)

    if not new_alerts_this_cycle:
        print("‚úÖ No new significant alerts to send for this cycle.")
    else:
        level_order = ["CRITICAL", "WARNING", "ALERT", "WATCHLIST", "INFO"]
        highest_level_in_news = "INFO"
        for level in level_order:
            if any(alert['level'] == level for alert in new_alerts_this_cycle):
                highest_level_in_news = level
                break

        cooldown_minutes = LEVEL_COOLDOWN_MINUTES.get(highest_level_in_news, 120)
        last_sent_time_for_level = cooldown_data.get("last_sent_level", {}).get(highest_level_in_news, 0)

        if now_ts - last_sent_time_for_level >= cooldown_minutes * 60:
            print(f"üî• Found {len(new_alerts_this_cycle)} new alerts. Highest level: {highest_level_in_news}. Sending digest...")

            for alert in new_alerts_this_cycle:
                save_news_for_precious(alert)
            print(f"‚úÖ Wrote/Updated {len(new_alerts_this_cycle)} items to signal file for downstream systems.")

            news_by_level = defaultdict(list)
            for alert in new_alerts_this_cycle:
                news_by_level[alert['level']].append(alert)

            context_block = f"```B·ªëi c·∫£nh th·ªã tr∆∞·ªùng | Fear & Greed: {context.get('fear_greed', 'N/A')} | BTC.D: {context.get('btc_dominance', 'N/A')}% | Trend: {market_trend}```"
            news_blocks = []
            for level in level_order:
                if level in news_by_level:
                    level_emoji = {"CRITICAL": "üî¥", "WARNING": "‚ö†Ô∏è", "ALERT": "üì£", "WATCHLIST": "üëÄ", "INFO": "‚ÑπÔ∏è"}.get(level, "‚ÑπÔ∏è")
                    level_header = f"{level_emoji} **{level}**"
                    news_blocks.append(level_header)
                    
                    sorted_alerts = sorted(news_by_level[level], key=lambda x: abs(x['news_score']), reverse=True)
                    
                    for alert in sorted_alerts:
                        score = alert.get('news_score', 0)
                        sentiment_emoji = ''
                        if score > 0: sentiment_emoji = 'üöÄ'
                        elif score < 0: sentiment_emoji = 'üö®'

                        score_str = f"(ƒêi·ªÉm: {score})"
                        part = (f"- **[{alert['source_name']}] {alert['title']}** {sentiment_emoji}\n"
                                f"  *‚Ü≥ Nh·∫≠n ƒë·ªãnh:* {alert['suggestion']} {score_str} [Link](<{alert['url']}>)")
                        news_blocks.append(part)

            final_summary = generate_final_summary(new_alerts_this_cycle, market_trend)
            full_digest_message = (f"**üî• B·∫¢N TIN TH·ªä TR∆Ø·ªúNG - {datetime.now(VN_TZ).strftime('%H:%M')} üî•**\n\n"
                                   + context_block + "\n\n"
                                   + "\n".join(news_blocks)
                                   + f"\n\n{final_summary}")
            send_discord_alert(full_digest_message)

            sent_ids_this_cycle = [alert['id'] for alert in new_alerts_this_cycle]
            cooldown_data["last_sent_id"] = (cooldown_data.get("last_sent_id", []) + sent_ids_this_cycle)[-50:]
            
            updated_levels = set(alert['level'] for alert in new_alerts_this_cycle)
            for level in updated_levels:
                cooldown_data.get("last_sent_level", {})[level] = now_ts

            with open(COOLDOWN_TRACKER, 'w') as f:
                json.dump(cooldown_data, f, indent=2)
            print("‚úÖ Cooldown tracker file saved.")
        else:
            print(f"‚è≥ Digest skipped. Highest level '{highest_level_in_news}' is on cooldown.")

    print("--- Cycle Finished ---")

if __name__ == "__main__":
    main()
