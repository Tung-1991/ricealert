(venv) root@ricealert:~/ricealert$cat ml_report.py
# /root/ricealert/ml_report.py
# PHIÃŠN Báº¢N ULTIMATE (ÄÃƒ Sá»¬A Lá»–I): "Há»™i Äá»“ng ChuyÃªn Gia AI"
# TÃ¡c giáº£: Äá»‘i tÃ¡c láº­p trÃ¬nh & [TÃªn cá»§a báº¡n]
#
# CHANGELOG (Báº£n sá»­a lá»—i):
# - Sá»¬A Lá»–I Táº¢I MODEL: Viáº¿t láº¡i hoÃ n toÃ n hÃ m `load_all_models`. Giá» Ä‘Ã¢y nÃ³ sáº½ táº£i
#   tá»«ng loáº¡i model (LGBM, LSTM, Transformer) má»™t cÃ¡ch Ä‘á»™c láº­p. Náº¿u má»™t loáº¡i model
#   bá»‹ thiáº¿u file, nÃ³ sáº½ bá» qua loáº¡i Ä‘Ã³ nhÆ°ng váº«n tiáº¿p tá»¥c táº£i cÃ¡c loáº¡i khÃ¡c,
#   Ä‘áº£m báº£o há»‡ thá»‘ng khÃ´ng bá»‹ dá»«ng láº¡i má»™t cÃ¡ch vÃ´ lÃ½.
# - TÃNH Äá»˜C Láº¬P: Sao chÃ©p trá»±c tiáº¿p cÃ¡c hÃ m `add_features` vÃ  `create_sequences`
#   vÃ o file nÃ y vÃ  xÃ³a bá» dÃ²ng `from trainer import ...`. Äiá»u nÃ y lÃ m cho
#   ml_report.py trá»Ÿ nÃªn hoÃ n toÃ n Ä‘á»™c láº­p, trÃ¡nh cÃ¡c lá»—i tiá»m áº©n liÃªn quan Ä‘áº¿n import.
# - Cáº¢I THIá»†N LOG: ThÃªm cÃ¡c dÃ²ng print chi tiáº¿t hÆ¡n Ä‘á»ƒ báº¡n biáº¿t chÃ­nh xÃ¡c model nÃ o
#   Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng hoáº·c bá»‹ bá» qua.

import os
import sys
import json
import warnings
import requests
import joblib
import pandas as pd
import numpy as np
import ta
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from typing import Dict, List, Any

# --- Cáº¥u hÃ¬nh Ä‘á»ƒ giáº£m log rÃ¡c cá»§a TensorFlow ---
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "3")
warnings.filterwarnings("ignore", category=UserWarning)

# --- Táº£i cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t ---
import tensorflow as tf
tf.get_logger().setLevel("ERROR")
import lightgbm as lgb
from tensorflow.keras.models import load_model

# ==============================================================================
# âš™ï¸ Cáº¤U HÃŒNH & Háº°NG Sá»
# ==============================================================================
load_dotenv()

SYMBOLS = os.getenv("SYMBOLS", "ETHUSDT,BTCUSDT").split(",")
INTERVALS = os.getenv("INTERVALS", "1h,4h,1d").split(",")
WEBHOOK_URL = os.getenv("DISCORD_AI_WEBHOOK")

ENSEMBLE_WEIGHTS = {
    "lightgbm": 0.25,
    "lstm": 0.35,
    "transformer": 0.40
}

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
LOG_DIR = os.path.join(BASE_DIR, "ai_logs")
os.makedirs(LOG_DIR, exist_ok=True)

# ==============================================================================
# (Má»šI) CÃC HÃ€M HELPER ÄÆ¯á»¢C SAO CHÃ‰P VÃ€O Äá»‚ Äáº¢M Báº¢O TÃNH Äá»˜C Láº¬P
# ==============================================================================

def add_features(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    close, high, low, volume = out["close"], out["high"], out["low"], out["volume"]
    out['price'] = close
    out['vol_ma20'] = volume.rolling(window=20).mean()
    bb = ta.volatility.BollingerBands(close, window=20)
    out['bb_upper'], out['bb_lower'] = bb.bollinger_hband(), bb.bollinger_lband()
    out['bb_width'] = (out['bb_upper'] - out['bb_lower']) / (bb.bollinger_mavg() + 1e-9)
    macd = ta.trend.MACD(close)
    out['macd'], out['macd_signal'], out["macd_diff"] = macd.macd(), macd.macd_signal(), macd.macd_diff()
    for n in [14, 28, 50]:
        out[f'rsi_{n}'] = ta.momentum.rsi(close, window=n)
        out[f'ema_{n}'] = ta.trend.ema_indicator(close, window=n)
        out[f'dist_ema_{n}'] = (close - out[f'ema_{n}']) / (out[f'ema_{n}'] + 1e-9)
    out["adx"] = ta.trend.adx(high, low, close)
    out['atr'] = ta.volatility.average_true_range(high, low, close, window=14)
    out['cmf'] = ta.volume.chaikin_money_flow(high, low, close, volume, window=20)
    for n in [1, 2, 3, 5, 8, 13, 21]: out[f'pct_change_lag_{n}'] = close.pct_change(periods=n)
    out.replace([np.inf, -np.inf], np.nan, inplace=True)
    out.bfill(inplace=True); out.ffill(inplace=True); out.fillna(0, inplace=True)
    return out

def create_sequences(data: pd.DataFrame, feature_cols: list, seq_length: int):
    X = []
    # Chá»‰ cáº§n táº¡o X, khÃ´ng cáº§n y cho viá»‡c dá»± Ä‘oÃ¡n
    for i in range(len(data) - seq_length + 1):
        X.append(data[feature_cols].iloc[i:(i + seq_length)].values)
    return np.array(X)

# ==============================================================================
# ğŸ“š Lá»šP QUáº¢N LÃ "Há»˜I Äá»’NG CHUYÃŠN GIA AI" (ÄÃƒ Sá»¬A Lá»–I)
# ==============================================================================

class AIEnsemble:
    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.models = {}
        self.scalers = {}
        self.metas = {}
        self.sequence_length = 0

    def load_all_models(self, symbols: List[str], intervals: List[str]):
        """(Sá»¬A Lá»–I) Táº£i tá»«ng model má»™t cÃ¡ch Ä‘á»™c láº­p Ä‘á»ƒ tÄƒng tÃ­nh kiÃªn cÆ°á»ng."""
        print("ğŸ§  Äang táº£i 'Há»™i Ä‘á»“ng ChuyÃªn gia AI' vÃ o bá»™ nhá»›...")
        loaded_count = 0
        total_pairs = 0
        for sym in symbols:
            for iv in intervals:
                total_pairs += 1
                key = f"{sym}_{iv}"
                self.models[key] = {}

                meta_path = os.path.join(self.data_dir, f"meta_{sym}_{iv}.json")
                scaler_path = os.path.join(self.data_dir, f"scaler_{sym}_{iv}.pkl")

                try:
                    with open(meta_path, 'r') as f:
                        self.metas[key] = json.load(f)
                    self.scalers[key] = joblib.load(scaler_path)
                    if not self.sequence_length:
                        self.sequence_length = self.metas[key].get('sequence_length', 60)
                except FileNotFoundError:
                    print(f"     - âš ï¸ KhÃ´ng tÃ¬m tháº¥y file ná»n táº£ng (meta/scaler) cho {key}. Bá» qua.")
                    continue

                model_loaded_for_pair = False
                # LightGBM
                try:
                    self.models[key]['lgbm_clf'] = joblib.load(os.path.join(self.data_dir, f"model_{sym}_lgbm_clf_{iv}.pkl"))
                    self.models[key]['lgbm_reg'] = joblib.load(os.path.join(self.data_dir, f"model_{sym}_lgbm_reg_{iv}.pkl"))
                    model_loaded_for_pair = True
                except FileNotFoundError: pass

                # LSTM
                try:
                    self.models[key]['lstm_clf'] = load_model(os.path.join(self.data_dir, f"model_{sym}_lstm_clf_{iv}.h5"), compile=False)
                    self.models[key]['lstm_reg'] = load_model(os.path.join(self.data_dir, f"model_{sym}_lstm_reg_{iv}.h5"), compile=False)
                    model_loaded_for_pair = True
                except (FileNotFoundError, IOError): pass

                # Transformer
                try:
                    self.models[key]['transformer_clf'] = load_model(os.path.join(self.data_dir, f"model_{sym}_transformer_clf_{iv}.h5"), compile=False)
                    self.models[key]['transformer_reg'] = load_model(os.path.join(self.data_dir, f"model_{sym}_transformer_reg_{iv}.h5"), compile=False)
                    model_loaded_for_pair = True
                except (FileNotFoundError, IOError): pass

                if model_loaded_for_pair:
                    loaded_count += 1
                    print(f"  -> âœ… ÄÃ£ táº£i model cho {key}")
                else:
                    print(f"  -> âŒ KhÃ´ng cÃ³ file model nÃ o cho {key}")


        print(f"âœ… ÄÃ£ táº£i thÃ nh cÃ´ng model cho {loaded_count}/{total_pairs} cáº·p coin/khung giá».")

    def predict(self, symbol: str, interval: str, df: pd.DataFrame) -> Dict[str, Any]:
        key = f"{symbol}_{interval}"
        if key not in self.models or not self.models[key]:
            return None

        meta = self.metas[key]
        scaler = self.scalers[key]
        features_to_use = meta['features']

        df_features = add_features(df)
        current_data = df_features.reindex(columns=features_to_use, fill_value=0)
        current_data_scaled = scaler.transform(current_data)
        current_data_scaled_df = pd.DataFrame(current_data_scaled, columns=features_to_use, index=current_data.index)

        opinions = {}

        # LightGBM
        if 'lgbm_clf' in self.models[key]:
            latest_features_lgbm = current_data.iloc[[-1]]
            lgbm_clf_prob = self.models[key]['lgbm_clf'].predict_proba(latest_features_lgbm)[0]
            lgbm_reg_pred = self.models[key]['lgbm_reg'].predict(latest_features_lgbm)[0]
            opinions['lightgbm'] = self._format_prediction(lgbm_clf_prob, lgbm_reg_pred, meta, df_features.iloc[-1]['atr'])

        # Dá»¯ liá»‡u chuá»—i cho LSTM & Transformer
        seq_to_predict = None
        if 'lstm_clf' in self.models[key] or 'transformer_clf' in self.models[key]:
            # Chá»‰ táº¡o chuá»—i náº¿u cáº§n
            latest_sequence_scaled = create_sequences(current_data_scaled_df, features_to_use, self.sequence_length)
            if len(latest_sequence_scaled) > 0:
                seq_to_predict = latest_sequence_scaled[[-1]]

        if seq_to_predict is not None:
            # LSTM
            if 'lstm_clf' in self.models[key]:
                lstm_clf_prob = self.models[key]['lstm_clf'].predict(seq_to_predict, verbose=0)[0]
                lstm_reg_pred = self.models[key]['lstm_reg'].predict(seq_to_predict, verbose=0)[0][0]
                opinions['lstm'] = self._format_prediction(lstm_clf_prob, lstm_reg_pred, meta, df_features.iloc[-1]['atr'])
            # Transformer
            if 'transformer_clf' in self.models[key]:
                trans_clf_prob = self.models[key]['transformer_clf'].predict(seq_to_predict, verbose=0)[0]
                trans_reg_pred = self.models[key]['transformer_reg'].predict(seq_to_predict, verbose=0)[0][0]
                opinions['transformer'] = self._format_prediction(trans_clf_prob, trans_reg_pred, meta, df_features.iloc[-1]['atr'])

        if not opinions: return None

        final_prob_buy, final_prob_sell, final_pct = 0.0, 0.0, 0.0
        valid_opinions = {k: v for k, v in opinions.items() if v is not None}
        total_weight = sum(ENSEMBLE_WEIGHTS[k] for k in valid_opinions)
        if total_weight == 0: return None # KhÃ´ng cÃ³ model nÃ o há»£p lá»‡

        for name, opinion in valid_opinions.items():
            weight = ENSEMBLE_WEIGHTS[name] / total_weight
            final_prob_buy += opinion['prob_buy'] * weight
            final_prob_sell += opinion['prob_sell'] * weight
            final_pct += opinion['pct'] * weight

        final_result = {
            "symbol": symbol, "interval": interval,
            "prob_buy": round(final_prob_buy, 2), "prob_sell": round(final_prob_sell, 2),
            "pct": round(final_pct, 4), "price": df_features.iloc[-1]['price'],
            "debug_info": {
                "ensemble_method": "Weighted Average", "weights": ENSEMBLE_WEIGHTS,
                "expert_opinions": opinions
            }
        }
        final_result.update(self._classify_level(final_result))
        return final_result

    def _format_prediction(self, prob, reg_pred, meta, current_atr):
        # prob[0] = sell, prob[1] = hold, prob[2] = buy
        prob_sell = prob[0] * 100
        prob_buy = prob[2] * 100
        pct = (reg_pred * current_atr / meta.get('atr_factor_threshold', 0.75)) * 100
        return {"prob_buy": prob_buy, "prob_sell": prob_sell, "pct": pct}

    def _classify_level(self, result: Dict) -> Dict:
        pb, ps, pct = result['prob_buy'], result['prob_sell'], result['pct']
        if pb > 70 and pb > ps * 2: return {"level": "STRONG_BUY", "sub_level": "STRONG_BUY"}
        if ps > 70 and ps > pb * 2: return {"level": "PANIC_SELL", "sub_level": "PANIC_SELL"}
        if pb > 60 and pb > ps * 1.5: return {"level": "BUY", "sub_level": "BUY"}
        if ps > 60 and ps > pb * 1.5: return {"level": "SELL", "sub_level": "SELL"}
        if pb > 55: return {"level": "WEAK_BUY", "sub_level": "WEAK_BUY"}
        if ps > 55: return {"level": "WEAK_SELL", "sub_level": "WEAK_SELL"}
        dz = {"1h": 0.4, "4h": 0.8, "1d": 1.2}.get(result['interval'], 0.8)
        if abs(pct) < dz:
            sub = "HOLD_NEUTRAL"
            if pb > ps + 5: sub = "HOLD_BULLISH"
            elif ps > pb + 5: sub = "HOLD_BEARISH"
            return {"level": "HOLD", "sub_level": sub}
        sub = "AVOID_UNCERTAIN"
        if pb > 35 and ps > 35: sub = "AVOID_CONFLICT"
        return {"level": "AVOID", "sub_level": sub}

# ==============================================================================
# ğŸš€ VÃ’NG Láº¶P CHÃNH & BÃO CÃO (TÆ°Æ¡ng tá»± phiÃªn báº£n trÆ°á»›c)
# ==============================================================================

def get_price_data_for_prediction(symbol: str, interval: str, limit: int) -> pd.DataFrame:
    url = "https://api.binance.com/api/v3/klines"
    params = {"symbol": symbol, "interval": interval, "limit": limit}
    try:
        resp = requests.get(url, params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        if not isinstance(data, list) or not data: return pd.DataFrame()
        df = pd.DataFrame(data, columns=["timestamp","open","high","low","close","volume","c1","c2","c3","c4","c5","c6"]).iloc[:, :6]
        df.columns = ["timestamp","open","high","low","close","volume"]
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
        df.set_index("timestamp", inplace=True)
        return df.astype(float)
    except Exception as e:
        print(f"[ERROR] Lá»—i get_price_data cho {symbol} {interval}: {e}")
        return pd.DataFrame()

def atomic_write_json(filepath: str, data: Dict):
    temp_filepath = filepath + ".tmp"
    with open(temp_filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    os.replace(temp_filepath, filepath)

def format_discord_report(all_results: List[Dict]) -> str:
    if not all_results: return "KhÃ´ng cÃ³ dá»¯ liá»‡u AI Ä‘á»ƒ bÃ¡o cÃ¡o."
    results_by_symbol = {}
    for res in all_results:
        if res['symbol'] not in results_by_symbol:
            results_by_symbol[res['symbol']] = []
        results_by_symbol[res['symbol']].append(res)
    buy_signals = sum(1 for r in all_results if "BUY" in r.get('level', ''))
    sell_signals = sum(1 for r in all_results if "SELL" in r.get('level', ''))
    market_sentiment = "TRUNG Láº¬P ğŸ˜"
    if buy_signals > sell_signals * 1.5: market_sentiment = "Láº C QUAN ğŸ“ˆ"
    elif sell_signals > buy_signals * 1.5: market_sentiment = "BI QUAN ğŸ“‰"
    now_vn = datetime.now(timezone(timedelta(hours=7)))
    header = (f"ğŸ“Š **Tá»•ng quan Thá»‹ trÆ°á»ng AI - {now_vn.strftime('%H:%M (%d/%m/%Y)')}**\n"
              f"Nhiá»‡t káº¿ thá»‹ trÆ°á»ng: **{market_sentiment}**\n")
    level_icons = {"STRONG_BUY": "ğŸš€", "BUY": "âœ…", "WEAK_BUY": "ğŸŸ¢", "PANIC_SELL": "ğŸ†˜", "SELL": "âŒ", "WEAK_SELL": "ğŸ”»", "HOLD_BULLISH": "ğŸ“ˆ", "HOLD_BEARISH": "ğŸ“‰", "HOLD_NEUTRAL": "ğŸ¤”", "AVOID_CONFLICT": "âš”ï¸", "AVOID_UNCERTAIN": "â“"}
    chunks = []
    for symbol, results in sorted(results_by_symbol.items()):
        price = results[0].get('price', 0)
        chunk_header = f"\n--- **{symbol}** | GiÃ¡: `{price:,.4f}` ---"
        chunk_body = []
        for res in sorted(results, key=lambda x: ['1h', '4h', '1d'].index(x['interval'])):
            iv, sub_level, pct = res['interval'], res.get('sub_level', 'N/A'), res.get('pct', 0)
            icon = level_icons.get(sub_level, "â”")
            line1 = f"  â€¢ **[{iv}]** {icon} **{sub_level.replace('_', ' ')}** | Dá»± Ä‘oÃ¡n: **{pct:+.2f}%**"
            opinions = res.get('debug_info', {}).get('expert_opinions', {})
            details = []
            if 'lightgbm' in opinions: details.append(f"LGBM: {opinions['lightgbm']['pct']:.2f}%")
            if 'lstm' in opinions: details.append(f"LSTM: {opinions['lstm']['pct']:.2f}%")
            if 'transformer' in opinions: details.append(f"Tran: {opinions['transformer']['pct']:.2f}%")
            line2 = f"    ```- {', '.join(details)}```" if details else ""
            chunk_body.append(f"{line1}\n{line2}" if line2 else line1)
        chunks.append(f"{chunk_header}\n" + "\n".join(chunk_body))
    return header + "".join(chunks)

def send_discord_message(content: str):
    if not WEBHOOK_URL: return
    max_len = 1900
    for i in range(0, len(content), max_len):
        chunk = content[i:i+max_len]
        try: requests.post(WEBHOOK_URL, json={"content": chunk}, timeout=10)
        except Exception as e: print(f"[ERROR] Lá»—i gá»­i Discord: {e}")

if __name__ == "__main__":
    print("--- Báº¯t Ä‘áº§u chu trÃ¬nh ML Report (Ultimate Edition - Fixed) ---")
    ensemble = AIEnsemble(DATA_DIR)
    ensemble.load_all_models(SYMBOLS, INTERVALS)
    if not any(ensemble.models.values()):
        print("âŒ KhÃ´ng cÃ³ model nÃ o Ä‘Æ°á»£c táº£i. Dá»«ng chÆ°Æ¡ng trÃ¬nh.")
        sys.exit(1)
    all_predictions = []
    for sym in SYMBOLS:
        for iv in INTERVALS:
            print(f"  -> Äang dá»± Ä‘oÃ¡n cho {sym} [{iv}]...")
            data_limit = ensemble.sequence_length + 200
            df_new = get_price_data_for_prediction(sym, iv, limit=data_limit)
            if df_new is None or len(df_new) < ensemble.sequence_length + 50:
                print(f"     - âš ï¸ Thiáº¿u dá»¯ liá»‡u má»›i cho {sym} [{iv}]. Bá» qua.")
                continue
            prediction = ensemble.predict(sym, iv, df_new)
            if prediction:
                json_filepath = os.path.join(LOG_DIR, f"{sym}_{iv}.json")
                atomic_write_json(json_filepath, prediction)
                all_predictions.append(prediction)
    print("\nğŸ“Š Äang táº¡o bÃ¡o cÃ¡o Discord...")
    discord_report = format_discord_report(all_predictions)
    send_discord_message(discord_report)
    print("âœ… ÄÃ£ gá»­i bÃ¡o cÃ¡o lÃªn Discord.")
    print("\nğŸ¯ğŸ¯ğŸ¯ Chu trÃ¬nh ML Report Ä‘Ã£ hoÃ n táº¥t. ğŸ¯ğŸ¯ğŸ¯")
